# Recent Trends in NLP

## Conquering on Basic NLP

이전에 다루었던 대로, 인공지능의 다른 분야에 비해서 NLP는 가장 늦게 빛을 보기 시작하였다고 하였지만, 여러 task에 deep learning을 적용하려는 시도는 많이 이루어졌고, 진전은 있었습니다. 2010년에는 RNN을 활용하여 language modeling을 시도\[Mikolov et al.2010\]하여 기존의 n-gram 기반의 language model의 한계를 극복하려 하였습니다. 그리하여 기존의 n-gram 방식과의 interpolation을 통해서 더 나은 성능의 language model을 만들어낼 수 있었지만, 기존에 langauge model이 사용되던 음성인식과 기계번역에 적용되기에는 구조적인 한계\(Weighted Finite State Transeducer, WFST의 사용\)로 인해서 더 큰 성과를 거둘 수는 없었습니다. -- 애초에 n-gram 기반 언어모델의 한계는 WFST에 기반하였기 때문이라고도 볼 수 있습니다. 닭이 먼저냐, 달걀이 먼저냐의 문제와 같음.

![](/assets/intro-rnnlm.png)

## Flourish of NLG

## Breakthough with Attention, and Future

## Convergence with Reinforcement Learning



