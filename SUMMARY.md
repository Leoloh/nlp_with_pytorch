# Summary

* [소개글](README.md)
* [To Do & Milestone](to-do-and-milestone.md)
* Natural Language Understanding with Deeplearning
  * 소개
  * 왜 중요한가?
  * 왜 어려운 것인가?
  * Deeplearning
  * History
* PyTorch 소개
  * 소개
  * 설치 방법
  * TorchText 소개
  * 예제
* Word Sense Disambiguation \(WSD\)
  * [소개](word-sense-disambiguation.md)
  * Bayes Theorem
  * Naive Bayes
  * Selectional Preference
  * WordNet
* Preprocessing
  * [전처리 개요](c804-cc98-b9ac-ac1c-c694.md)
  * Corpus 수집
  * 정제
  * Tokenization \(POS Tagging\)
* Word Embedding Vector
  * 개요
  * [One-hot encoding](one-hot-encoding.md)
  * [Previous methods](previous-methods.md)
  * Skip-gram & CBOW
  * GloVe
  * FastText
  * Further methods
* RNN
  * RNN
  * LSTM
  * GRU
  * Gradient Clipping
* Sequential Problem
  * 문제 정의
  * Hidden Markov Model
  * Conditional Random Field
  * Using Deeplearning
  * Named Entity Recognition
  * Part of Speech Tagging
* Language Modeling
  * [언어모델 개요](c5b8-c5b4-baa8-b378-ac1c-c694.md)
  * [Perpexity](perpexity.md)
  * [n-gram](n-gram.md)
  * [NNLM](nnlm.md)
* Text Classification
  * 소개
  * Using RNN
  * Using CNN
  * Using RNN
  * Unsupervised Text Classification
* Speech Recognition
  * 음성인식 소개 및 역사
  * 성능 평가 방법
  * WFST 방식
  * End2end deeplearning
* Neural Machine Translation
  * [기계번역 개요 및 역사](ae30-acc4-bc88-c5ed-ac1c-c694-bc0f-c5ed-c0ac.md)
  * [성능 평가 방법](c131-b2a5-d3c9-ac00-bc29-bc95.md)
  * [Seq2seq](seq2seq.md)
  * [Attention](attention.md)
  * [Input Feeding](input-feeding.md)
  * Beam Search
  * Fully Convolutional Seq2seq
  * Transformer
* Reinforcement Learning for NMT
  * 강화학습 개요
  * Policy Gradient
  * Supervised NMT
  * Unsupervised NMT
* Future Work
  * Memory Augmented Neural Network
    * Text Summarization
    * Quetional Answering

