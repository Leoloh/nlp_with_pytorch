# N-gram

## 소개

이전 섹션에서 언어모델에 대해 소개를 간략히 했습니다. 하지만 만약 그 수식대로라면 우리는 확률들을 거의 구할 수 없을 것 입니다. 왜냐하면 비록 우리가 수천만~수억 문장을 인터넷에서 긁어 모았다고 하더라도, 애초에 출현할 수 있는 단어의 조합의 경우의 수는 무한대이기 때문입니다. 문장이 조금만 길어지더라도 Count를 구할 수 없어 분자가 0이 되어 버리거나, 심지어 분모가 0이 되어 버릴 것이기 때문입니다.

## Markov Assumption

따라서 이러한 문제를 해결하기 위해서 Markov Assumption을 도입합니다.

$$
P(x_i|x_1,x_2,...,x_{i-1})≈P(x_i|x_{i-k},...,x_{i-1})
$$

한마디로 이전에 출현한 모든 단어를 볼 필요 없이 앞에 $$ k $$ 개의 단어만 조건으로써 본다는 것입니다. 이렇게 simplify 한 표현을 통해서 확률을 approximation 하겠다는 것 입니다. 보통 $$ k $$는 0에서 3의 값을 갖게 됩니다. 즉, $$ k = 2 $$ 일 경우에는 앞 단어 2개를 참조하여 $$ x_i $$ 단어의 확률을 근사하여 나타내게 됩니다.

$$
P(x_i|x_{i-2},x_{i-1}) 
$$

이를 응용하여 문장에 대한 확률도 다음과 같이 표현 할 수 있습니다.

$$
P(x_1,x_2,...,x_i)=\prod_{j}{P(x_j|x_{j-k},...,x_{j-1})}
$$

이때, $$ n=k+1 $$으로 ***n-gram***이라고 부릅니다.

| k | n-gram | 명칭 |
|-|-|-|
| 0 | 1-gram | uni-gram |
| 1 | 2-gram | bi-gram |
| 2 | 3-gram | tri-gram |

위 테이블과 같이 3-gram 까지는 tri-gram이라고 읽지만 4-gram 부터는 그냥 four-gram 이라고 읽습니다.

## 한계

## Smoothing

## Back-off