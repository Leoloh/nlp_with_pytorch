# N-gram

## 소개

이전 섹션에서 언어모델에 대해 소개를 간략히 했습니다. 하지만 만약 그 수식대로라면 우리는 확률들을 거의 구할 수 없을 것 입니다. 왜냐하면 비록 우리가 수천만~수억 문장을 인터넷에서 긁어 모았다고 하더라도, 애초에 출현할 수 있는 단어의 조합의 경우의 수는 무한대이기 때문입니다. 문장이 조금만 길어지더라도 Count를 구할 수 없어 분자가 0이 되어 버리거나, 심지어 분모가 0이 되어 버릴 것이기 때문입니다.

## Markov Assumption

따라서 이러한 문제를 해결하기 위해서 Markov Assumption을 도입합니다.

$$
P(x_i|x_1,x_2,...,x_{i-1})≈P(x_i|x_{i-k},...,x_{i-1})
$$

한마디로 이전에 출현한 모든 단어를 볼 필요 없이 앞에 $$ k $$ 개의 단어만 조건으로써 본다는 것입니다. 보통 $$ k $$는 0에서 3의 값을 갖게 됩니다. 즉, $$ k = 2 $$ 일 경우에는 앞 단어 2개를 참조하여 $$ x_i $$ 단어의 확률을 나타내게 됩니다.

$$ 
P(x_i|x_{i-2},x_{i-1}) 
$$

이를 응용하여 문장에 대한 확률도 다음과 같이 표현 할 수 있습니다.

$$ 
P(x_1,x_2,...,x_i)=
$$

## 한계

## Smoothing

## Back-off