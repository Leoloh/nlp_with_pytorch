# How to evaluate Language Model?

좋은 언어모델이란 실제 우리가 쓰는 언어에 대해서 최대한 비슷하게 확률 분포를 근사하는 모델(또는 파라미터, $$\theta$$)이 될 것입니다. 즉, 문장을 잘 예측 해 낼 수록 좋은 언어모델이라고 할 수 있습니다. 문장을 잘 예측 한다는 것은, 다르게 말하면 주어진 문장이 언어모델에서 높은 확률을 가진다고 할 수 있습니다. 또는 다음 단어를 예측 할 때에 확률이 높을 수록(또는 불확실성이 낮을수록) 더욱 좋은 언어모델이라고 할 수 있을 것 입니다. 그럼 이번 섹션은 언어모델의 성능을 평가하는 방법에 대해서 다루도록 하겠습니다.

## Perplexity

Perplexity 측정 방법은 explicit evaluation 방법의 하나입니다. 줄여서 PPL이라고 부르기도 합니다. PPL을 이용하여 테스트 문장에 대해서 언어모델을 이용하여 점수를 구하고 언어모델의 성능을 측정합니다. 문장에 대한 확률을 구하고 문장의 길이에 대해서 normalization 합니다. 수식은 아래와 같습니다.


$$
PPL(w_1,w_2,\cdots,w_n)=P(w_1,w_2,\cdots,w_N)^{-\frac{1}{N}}
$$



$$
=\sqrt[N]{\frac{1}{P(w_1,w_2,\cdots,w_N)}}
$$


Chain rule에 의해서


$$
=\sqrt[N]{\frac{1}{\prod_{i=1}^{N}{P(w_i|w_1,\cdots,w_{i-1})}}}
$$


n-gram이 적용 될 경우,


$$
=\sqrt[N]{\frac{1}{\prod_{i=1}^{N}{P(w_i|w_{i-n+1},\cdots,w_{i-1})}}}
$$


문장이 길어지게 되면 문장의 확률은 당연히 굉장히 작아지게 됩니다. 따라서 우리는 문장의 길이로 제곱근을 취해주어 normalization을 해 주는 것을 볼 수 있습니다. 실제 테스트 문장에 대해서 확률을 높게 예측할 수록 좋은 언어모델인 만큼, 해당 테스트 문장에 대한 PPL이 작을 수록 좋은 언어모델이라고 할 수 있습니다. 즉, _**PPL은 수치가 낮을수록 좋습니다.**_ 또한, n-gram이 \(n이\) 클 수록 보통 더 낮은 PPL을 보여주기도 합니다.

![](/assets/lm_rolling_dice.png)

단순히 수식을 설명하기보다 좀 더 개념을 짚고 넘어가도록 해 보겠습니다. Perplexity의 수치를 해석하는 방법에 대해서라고 할 수 있습니다. 예를 들어 우리가 1부터 6까지의 6개의 숫자만을 가지고 수열을 만들어낸다고 해 보겠습니다. 그리고 1부터 6까지 6개의 숫자의 출현 확률은 모두 같다고 가정하겠습니다. 즉, uniform distribution, 주사위를 던지는 것과 같습니다. 그럼 임의의 테스트 수열에 대한 perplexity는 아래와 같습니다.


$$
PPL(x)=({\frac{1}{6}}^{N})^{-\frac{1}{N}}=6
$$


매 time-step 가능한 가짓수인 6개와 같은 수치가 PPL로 나왔습니다. 즉, PPL은 우리가 뻗어나갈 수 있는 branch\(가지\)의 숫자를 의미하기도 합니다. 다른 예를 들어 만약 20,000개의 어휘로 이루어진 뉴스 기사에 대해서 PPL을 측정한다고 하였을 때, 단어의 출현 확률이 모두 동일하다면 PPL은 20,000이 될 것입니다. 하지만 3-gram을 사용한 언어모델을 만들어 측정한 PPL이 30이 나왔다면, 우리는 이 언어모델을 통해 해당 신문기사에서 단어마다 다음 단어로 넘어갈 때 평균적으로 30개의 후보로 간추릴 수 있다는 얘기가 됩니다. 따라서 우리는 perplexity를 통해서 언어모델의 성능을 측정할 뿐만 아니라 가늠 해 볼 수도 있습니다.

