# Evaluation for MT

번역기의 성능을 평가하는 방법은 크게 두 가지로 나눌 수 있습니다. 정성적(implicit) 평가와 정량적(explicit) 평가 방식입니다.

## 1. Implicit Evaluation

정성평가 방식은 보통 사람이 번역된 문장을 채점하는 형태로 이루어집니다. 사람은 선입견 등이 채점하는데 있어서 방해요소로 작용될 수 있기 때문에, 보통은 blind test를 통해서 채점합니다. 이를 위해서 여러개의 다른 알고리즘을 통해 (또는 경쟁사의) 여러 번역결과를 누구의 것인지 밝히지 않은 채, 채점하여 우열을 가립니다. 이 방식은 가장 정확하다고 할 수 있지만, 자원과 시간이 많이 드는 단점이 있습니다.

## 2. Explicit Evalution

위의 단점 때문에, 보통은 자동화 된 정량평가를 주로 수행합니다. 두 평가를 모두 주기적으로 수행하되, 정성평가의 평가 주기를 좀 더 길게 가져가거나, 무언가 확실한 성능의 jump가 이루어졌을 때 수행하는 편 입니다.

### a. Cross Entropy and Perplexity

Neural Machine Translation task도 기본적으로 어떤 단어를 pick 하는 작업이기 때문에 기본적으로 classification task에 속합니다. 따라서 ***Cross Entropy***를 Loss function으로 사용합니다. 

$$
H(p,q)=-{\sum_{\forall x}{p(x)\log{q(x)}}}
$$

위의 식은 기본 Cross Entropy 수식입니다. 이것을 우리의 번역 모델($$ M_\theta $$)에 적용하여 보면 아래와 같습니다.

$$
L= -\frac{1}{|Y|}\sum_{y \in Y}{P(y) \log P_\theta(y)}
$$

여기서 $$ P(y) $$는 정답(ground-truth)이므로 항상 $$ 1 $$입니다. 그러므로 아래와 같이 됩니다.

$$
L= -\frac{1}{|Y|}\sum_{y \in Y}{\log P_\theta(y)}
$$

$$
=\log{((\prod_{y \in Y}{P_\theta(y)})^{-\frac{1}{|Y|}})}
$$

$$
=\log{(\sqrt[|Y|]{\frac{1}{\prod_{y \in Y}{P_\theta(y)}}})}
$$

그런데 재미있는 점은 이전 챕터 Language Modeling 할 때 성능평가 지표로써 사용했던 Perplexity가 Cross Entropy와 밀접한 관련이 있다는 것 입니다. 이전 챕터에서 다루었던 PPL (Perplexity) 수식을 떠올려보겠습니다.

$$
PPL(W)=P(w_1, w_2, \cdots, w_N)^{-\frac{1}{N}}
=\sqrt[N]{\frac{1}{P(w_1,w_2,\cdots,w_N)}}
$$

$$
By~chain~rule,
$$

$$
PPL(W)=\sqrt[N]{\prod_{i=1}^{N}{\frac{1}{P(w_i|w_1,\cdots,w_{i-1})}}}
$$

앞서 정리했던 Cross Entropy와 수식이 비슷함을 알 수 있습니다. ***PPL***과 ***Cross Entropy***의 관계는 아래와 같습니다.

$$
PPL = \exp(Cross~Entropy)
$$

따라서, 우리는 Cross Entropy를 통해 얻은 Loss 값에 exponential을 취함으로써, PPL을 얻어 번역기의 성능을 나타낼 수 있습니다.

### b. BLEU

위의 PPL은 우리가 사용하는 Loss function과 직결되어 바로 알 수 있는 간편함이 있지만, 실제 번역기의 성능과 완벽한 비례관계에 있다고 할 수는 없습니다. Cross Entropy의 수식을 해석 해 보면, 각 time-step 별 실제 정답에 해당하는 단어의 확률만 채점하기 때문입니다.

|원문|I|love|to|go|to|school|.|
|-|-|-|-|-|-|-|-|
|index|0|1|2|3|4|5|6|
|정답|나는|학교에|가는|것을|좋아한다|.||
|번역1|학교에|가는|것을|좋아한다|나는|.||
|번역2|나는|오락실에|가는|것을|싫어한다|.||

예를 들어, 번역1은 cross entropy loss에 의하면 매우 높은 loss값을 가집니다. 하지만 번역2는 번역1에 비해 완전 틀린 번역이지만 loss가 훨씬 낮을 겁니다. 따라서 실제 번역문의 품질과 cross entropy 사이에는 괴리가 있습니다. 이러한 간극을 줄이기 위해 여러가지 방법들이 제시되었습니다 -- [METEOR](https://en.wikipedia.org/wiki/METEOR), [BLEU](https://en.wikipedia.org/wiki/BLEU). 이번 섹션은 그 중 가장 널리 쓰이는 BLEU에 대해 짚고 넘어가겠습니다.