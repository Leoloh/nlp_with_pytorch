# One-hot encoding

단어는 discrete한 심볼(symbol)로 그 내부의 의미는 유사성이 있을 수 있지만, 겉 형태는 다른 경우가 많습니다. (동형이의어 제외) 따라서 가장 기본적으로 단어를 벡터로 나타내는 방법은 one-hot 인코딩(encoding)이라는 방식 입니다. 이 방식은 말 그대로 단 한개의 1과 나머지 수많은 0들로 표현된 인코딩 방식을 뜻합니다. One-hot 인코딩 벡터의 차원은 보통 전체 어휘(vocabulary)의 갯수가 되며, 당연히 보통 그 숫자는 매우 큰 숫자가 됩니다. (대략 30,000~100,000)

$$
v\in\mathbb{R}^{|V|},\text{ where }v\text{ is one-hot vector and }|V|\text{ is vocabulary size.}
$$

따라서 전체 단어에 대해서 one-hot 벡터를 구성한다면 아래와 같은 형태가 될 것 입니다.

|단어|사전에서의 순서(index)|One-hot 벡터|
|-|-|-|
|...|||
|강아지|8|$0,0,0,0,0,0,0,1,0,0,0,\cdots,0$|
|개|9|$0,0,0,0,0,0,0,0,1,0,0,\cdots,0$|
|고양이|10|$0,0,0,0,0,0,0,0,0,1,0,\cdots,0$|
|구렁이|11|$0,0,0,0,0,0,0,0,0,0,1,\cdots,0$|
|...|||
|하마|20,567|$0,\cdots,0,0,0,0,0,0,0,0,0,0,1$|
|...|||

위처럼 사전(dictionalry)내의 각 단어를 one-hot encoding 방식을 통해 벡터로 나타낼 수 있습니다. 문제는 표현하는 정보에 비해 벡터의 차원이 너무 커진 것 입니다. 수만개의 단어 중 하나의 단어를 표현하기 위해서 단어수 만큼의 차원의 벡터에서 하나의 차원을 제외하고 모두 0으로 채웠기 때문입니다. 즉, 하나의 차원을 제외한 모둔 차원이 0으로 채워진 벡터 입니다. 이러한 벡터를 우리는 sparse vector라고 부릅니다.

## Curse of dimensionality

문제는 이런 sparse vector는 기계학습에 있어서 매우 큰 장벽으로 작용한다는 점 입니다. 예를 들어, 점보를 표현하는데 훨씬 큰 차원이 사용되었다면 작은 차원으로 같은 정보를 표현한 것에 비해서, 상대적으로 같은 크기의 공간에 표현되는 정보는 훨씬 더 적을 것이기 때문입니다.

![차원의 저주: 차원이 높을 수록 같은 정보를 표현하는데 불리합니다.](picture)

우리는 이런 문제를 차원의 저주(curse of dimensionailty)라고 부릅니다.

## Similarity

One-hot encoding을 통해 sparse vector로 표현된 워드 임베딩 벡터의 경우에는 또 다른 어려움이 있습니다. 바로 one-hot encoding을 통해서는 단어간의 유사도를 표현할 수 없다는 것 입니다. 우리는 임베딩 벡터간의 유사도 비교를 통해, 부족한 정보를 비슷한 다른 단어로부터 가져올 수 있을 것 입니다. 하지만 one-hot encoding을 통해 구한 임베딩 벡터의 경우에는 코사인 유사도(cosine similarity)가 항상 0일 수 밖에 없습니다. 또한, 유클리디안 거리(eucleadian distance)의 경우에도 모두가 같을 것 입니다. 따라서, one-hot encoding에 따르면, 컴퓨터는 '고양이'와 '강아지', 그리고 '개' 사이의 유사도를 구할 수 없을 것 입니다. 이것은 데이터를 통해서 학습을 수행하는 기계학습의 특성상 데이터가 많을 수록 유리한데 반해, 불리하게 작용할 수 밖에 없습니다. '강아지'에 대한 데이터가 적어 일반화(generalization)하기 어려울 때, '개'와 관련된 데이터로부터 도움을 받을 수 없을 것이기 때문입니다.

따라서 우리는 위와 같이 one-hot encoding의 한계를 실감할 수 밖에 없습니다. 따라서 우리는 차원의 저주로부터 벗어나기 위해 차원을 축소하여 단어를 표현해야 할 필요성을 느낍니다.